{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Data Prep Notebook\n",
    "## Data Source: ____\n",
    "This workbook uses a series of standard and configurable python code recipes to take a **structured** data source and prepare it for machine learning/data mining. <br>\n",
    "This workbook assumes some problem qualification and Exploratory Data Analysis on the data has already taken place. <br>\n",
    "The goal of this workbook is to provide a process and a starting point, so an analyst can spend less time prepping data, and more time fitting models! <br>\n",
    "\n",
    "**To do:**\n",
    "- Consider executing categorical feature encoding before handling null variables\n",
    "- HIGHLIGHT WHERE DATA ENTRY IS NEEDED (RED)\n",
    "- HIGHLIGHT CANDIDATE CELLS FOR PIPELINE GEN (#pipeline)\n",
    "<br>\n",
    "\n",
    "## Contents\n",
    "<a id='Section0'></a>\n",
    "[1. Setup: Goal for data prep](#Section1) <br>\n",
    "[2. Setup: Library and Data Source Import](#Section2) <br>\n",
    "[3. Setup: Identifying Cleaning Challenges](#Section3) <br>\n",
    "[4. Setup: Tidying and Deduplication](#Section4) <br>\n",
    "[5. Cleaning: Missing Value Management](#Section5) <br>\n",
    "[6. Cleaning: Numerical Features](#Section6) <br>\n",
    "[7. Cleaning: Boolean Features](#Section7) <br>\n",
    "[8. Cleaning: Date Features](#Section8) <br>\n",
    "[9. Cleaning: Categorical Features](#Section9) <br>\n",
    "[10. Cleaning: Other Features](#Section10) <br>\n",
    "[11. Evaluation of Prepared Data](#Section11) <br>\n",
    "[12. Construction of a Prep Pipeline](#Section12) <br>\n",
    "[13. References](#Section13) <br>\n",
    "\n",
    "\n",
    "### A note on Data Prep Processes\n",
    "Many experts recommend following a routine or process when prepping data, as it saves time and encourages best practice. <br>\n",
    "This workbook is follows the below process, which is further [documented here](http://bit.ly/2HOA18w) <br>\n",
    "<img align=\"left\" width=\"500\" height=\"1000\" src=\"images/Data Cleaning Process V1_Shared - Process Draft One.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Goal for data prep\n",
    "<a id='Section1'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "### State here the goal of your data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal goes here. <br>\n",
    "Quite a few decisions will be informed by this goal in this notebook - so it's important to have one, and to state it up front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Library and Data Source Import\n",
    "<a id='Section2'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section installs and imports some useful libraries for data prep, and provides some options on connecting with your data source. <br>\n",
    "**Note - if your data is not structured, you'll need to do that first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installing libraries if needed\n",
    "# import sys\n",
    "# # This workbook uses the Pandas Profiling library - you can install it on your local system using this code\n",
    "# !{sys.executable} -m pip install pandas_profiling\n",
    "# # This workbook may also use the fuzzywuzzy string matching library, which can be installed as follows\n",
    "# !{sys.executable} -m pip install fuzzywuzzy\n",
    "# ! {sys.executable} -m pip install python-Levenshtein\n",
    "# # This workbook's EXAMPLES uses the geopy library\n",
    "# !{sys.executable} -m pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This workbook uses the following libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns', 200) # Good for wide datasets - otherwise it will truncate the data in views like head\n",
    "import pandas_profiling # for exploration of datasets\n",
    "\n",
    "# for missing value processing\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# for numeric processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# for text processing\n",
    "import re \n",
    "import string\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Setting the seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "filepath_or_url = \"\"\n",
    "data_raw = pd.read_csv(filepath_or_url,low_memory=False) # import CSV\n",
    "# raw_data = pd.read_excel(filepath,sheet_name=\"\") # import XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating import\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import references:\n",
    "- [Pandas Input/Output Functions](https://pandas.pydata.org/pandas-docs/stable/api.html#input-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup: Identifying Cleaning Challenges\n",
    "<a id='Section3'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section includes:\n",
    "- Understanding default data types and columns\n",
    "- Understanding nulls\n",
    "- Using the Pandas Profiler to deep dive into variables\n",
    "- Evaluating some of the challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate column names, types, nulls, using info.\n",
    "data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate numerical and categorical top line items using describe\n",
    "data_raw.describe(include=[np.number]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate object and categorical items using describe - are they high in cardinality?!?\n",
    "data_raw.describe(include=[np.object,pd.Categorical]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"pandas profile report\" to enable efficient deep dives into all features.\n",
    "report = pandas_profiling.ProfileReport(data_raw)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can export the profile report if needed\n",
    "report.to_file(outputfile=\"raw_data_profile.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also see what variables the report recommends excluding based upon a correlation with other variables >0.9\n",
    "report.get_rejected_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally matplot.lib and Boolean Indexing can help deep dive on fields with strange distributions\n",
    "# data_raw[''].dropna().hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** <br>\n",
    "*Your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup: Tidying and Deduplication\n",
    "<a id='Section4'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section involves:\n",
    "- Standardizing column names.\n",
    "- Evaluating whether each row is a sample, and what transforms are needed to bring this about.\n",
    "- Testing for duplicates, and Identifying an index for each sample (where possible)\n",
    "- Sorting columns into the following **data types**: numerical, boolean, date, categorical, and other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per Chris Albion, it is best practice to treat a data frame as immutable, and to copy before manipulation (to protect against mistakes)\n",
    "\n",
    "# Copying the raw data file:\n",
    "data_prep_1 = data_raw.copy()\n",
    "\n",
    "# Standardizing column names to snake case:\n",
    "data_prep_1.columns = [c.replace(' ', '_') for c in data_prep_1.columns]\n",
    "data_prep_1.columns =  [c.lower() for c in data_prep_1.columns]\n",
    "data_prep_1.columns = [re.sub(r'\\W+','_',c) for c in data_prep_1.columns]\n",
    "\n",
    "# Reporting the resulting columns as a list for later reference\n",
    "data_prep_1.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the shape of the dataframe, and whether each row is a sample and each column is a variable\n",
    "# A random sample is thought to be a good way to do this.\n",
    "data_prep_1.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on tidiness evaluation:** <br>\n",
    "*Your notes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for duplicates - first across all features\n",
    "data_prep_1[data_prep_1.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for duplicates - then across index candidate\n",
    "index = \"\"\n",
    "data_prep_1[data_prep_1.duplicated(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing a duplicate from the index\n",
    "data_prep_2 = data_prep_1.drop_duplicates(subset=index, keep='first')\n",
    "\n",
    "# Optional if the previous step is not applicable\n",
    "# data_prep_2 = data_prep_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing on the new variable\n",
    "data_prep_3 = data_prep_2.set_index(index) # note this will delete the the original record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of irrelevant columns based on the profile report and other observations\n",
    "manual_exclude_list = []\n",
    "cols_exclude_total = manual_exclude_list\n",
    "cols_exclude_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification for manually excluded columns:** <br>\n",
    "*Your notes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_3['located_in'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting columns - defining the logic for the sort:\n",
    "def feature_sort(cols_num,cols_bool,cols_date,cols_cat,cols_other,cols_exclude_total):\n",
    "    for col in data_prep_3.columns:\n",
    "        if col not in cols_exclude_total + cols_num + cols_bool + cols_date + cols_cat + cols_other:\n",
    "            if col in data_prep_3.columns[(data_prep_3.dtypes == np.float64) | (data_prep_3.dtypes == np.float32)]:\n",
    "                cols_num.append(col)\n",
    "            elif (data_prep_3[col].nunique() == 2) or (\"true\" in data_prep_3[col].unique()) or (\"false\" in data_prep_3[col].unique()) \\\n",
    "            or (\"yes\" in data_prep_3[col].unique()) or (\"no\" in data_prep_3[col].unique()):\n",
    "                cols_bool.append(col)\n",
    "            elif 'date' in str(col):\n",
    "                cols_date.append(col)\n",
    "            elif data_prep_3[col].nunique() < data_prep_3.shape[0]/100: # Arbitrary limit\n",
    "                cols_cat.append(col)\n",
    "            else:\n",
    "                cols_other.append(col)\n",
    "    return cols_num,cols_bool,cols_date,cols_cat,cols_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptions can be handled by placing their values in the column names before executing the for loop\n",
    "cols_num = []\n",
    "cols_bool = []\n",
    "cols_date = []\n",
    "cols_cat = []\n",
    "cols_other = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the sort\n",
    "cols_num,cols_bool,cols_date,cols_cat,cols_other = feature_sort(cols_num,cols_bool,cols_date,cols_cat,cols_other,cols_exclude_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalaute the results of the sort\n",
    "[print(key+\" features:\",value,sep='\\n') for key,value in {\"cols_num\":cols_num,\"cols_bool\":cols_bool,\"cols_date\":cols_date,\\\n",
    "                                          \"cols_cat\":cols_cat,\"cols_other\":cols_other}.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Irrelevant features\n",
    "data_prep_4 = data_prep_3.drop(labels=cols_exclude_total,axis = 1)\n",
    "data_prep_4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References for section 4: \n",
    "##### Pandas Data Wrangling Functions\n",
    "[Cheat Sheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf) <br>\n",
    "*Credit - Irv Lustig, Princeton Consultants*\n",
    "##### Tidy Data\n",
    "**Image:** Definition of variables, observations, and values in Tidy Data <br>\n",
    "*Image Credit - R for Data Science (Hadley Wickham & Garrett Grolemund)* <br>\n",
    "<img align=\"left\" width=\"500\" height=\"500\" src=\"images/tidy data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning: Missing Value Management\n",
    "<a id='Section5'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section covers missing value management, and helps makes decisions on whether to drop values, impute values, or handle them otherwise.\n",
    "- Ensure we have detected all nans\n",
    "- Evaluate how many nans we have\n",
    "- Evaluate how important a feature is\n",
    "- Evaluate why the values are NaN - were the not recorded, or do they not exist?\n",
    "- Flag values for imputation during analysis\n",
    "- Drop values for the unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check category features for \"nulls\" hiding behind other values (a common gotcha!) \n",
    "[print(str(c)+' value counts'\\\n",
    "       ,data_prep_4[c].value_counts()\\\n",
    "       ,sep=\"\\n\") for c in cols_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a replacement dict of column names, and values, to replace with NaN's\n",
    "replace_dict = {\"column\":{\"value\":np.nan},\"column\":{\"value\":np.nan}}\n",
    "\n",
    "# Replace \"unknown\" values with nans\n",
    "data_prep_5 = data_prep_4.replace(to_replace=replace_dict)\n",
    "\n",
    "# Optional if the previous step is not applicable\n",
    "# data_prep_5 = data_prep_4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns with missing values by %\n",
    "data_prep_5.isnull().sum()\\\n",
    "    .apply(lambda x: (x/data_prep_4.shape[0])*100)\\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess impact of dropping all samples with missing values\n",
    "print(\"rows before drop: \" + str(data_prep_5.shape[0])\\\n",
    "      ,\"rows after drop: \" + str(data_prep_5.dropna().shape[0])\\\n",
    "      ,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are any missing features unimportant? If so, note them down and drop them\n",
    "cols_missing =[] # note them in this list\n",
    "cols_exclude_total = cols_exclude_total.append(cols_missing)\n",
    "data_prep_6=data_prep_5.drop(labels=cols_missing,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the values for the sample never recorded, or do they not exist?\n",
    "# This can be determined by reading the docs or through EDA\n",
    "not_recorded = []\n",
    "dont_exist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave not_recorded imputation for later (as only applies to numeric features, and can be impacted horrendously by outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples with dont_exist now\n",
    "print(data_prep_6.shape[0])\n",
    "data_prep_7 = data_prep_6.dropna(subset=dont_exist)\n",
    "print(data_prep_7.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort cols to adjust to anything dropped in this section\n",
    "# Running the sort\n",
    "cols_num,cols_bool,cols_date,cols_cat,cols_other = feature_sort(cols_num,cols_bool,cols_date,cols_cat,cols_other,cols_exclude_total)\n",
    "# Evalaute the results of the sort\n",
    "[print(key+\" features:\",value,sep='\\n') for key,value in {\"cols_num\":cols_num,\"cols_bool\":cols_bool,\"cols_date\":cols_date,\\\n",
    "                                          \"cols_cat\":cols_cat,\"cols_other\":cols_other}.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that there are other approaches that can be effective,\n",
    "# like fill_na (e.g. replace all NaNs with zero's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References for section 5\n",
    "[Imputing Missing Values with Means](https://chrisalbon.com/machine_learning/preprocessing_structured_data/impute_missing_values_with_means/) <br>\n",
    "[Handling Missing Values by Dan B](https://www.kaggle.com/dansbecker/handling-missing-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleaning: Numerical Features\n",
    "<a id='Section6'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section involves <br>\n",
    "- Outlier Management\n",
    "- Feature Transformation (e.g. rescaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers through Interquartile ranges\n",
    "\n",
    "# Defining a function to return the range and count of outliers\n",
    "# based on distance to interquartile ranges (<1.5*Q1, >1.5*Q3)\n",
    "\n",
    "def bounds_number_of_outliers(x): \n",
    "    q1, q3 = np.percentile(x, [25, 75]) \n",
    "    iqr = q3 - q1 \n",
    "    lower_bound = q1 - (iqr * 1.5) \n",
    "    upper_bound = q3 + (iqr * 1.5) \n",
    "    return lower_bound, upper_bound, len(np.where((x > upper_bound) | (x < lower_bound))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(c)+\" outlier lower bound, upper bound, and count:\", bounds_number_of_outliers(data_prep_7[c].dropna())\\\n",
    "       ,sep=\"\\n\")\\\n",
    " for c in cols_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on outliers:** <br>\n",
    "Your notes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To manage outliers, we can remove them, mark them, or transform them\n",
    "\n",
    "# removing them\n",
    "# data_prep_8[] = data_prep_7[(data_prep_7[''] < lower_bound) |\\\n",
    "# (data_prep_7[''] > upper_bound)]\n",
    "\n",
    "# Remove excluded features from columns\n",
    "# cols_exclude_total = cols_exclude_total+cols_missing\n",
    "# for l in [cols_num,cols_bool,cols_date,cols_cat,cols_other]:\n",
    "#     for c in l:\n",
    "#         if  c in l:\n",
    "#             l.remove(c)\n",
    "\n",
    "# marking them\n",
    "# data_prep_8['_outlier'] = np.where(data_prep_7[''] > upper_bound)\n",
    "\n",
    "# transforming them\n",
    "# data_prep_8 = data_prep_7.copy()\n",
    "# data_prep_8[\"log_of_diameter_breast_height\"] = \\\n",
    "# data_prep_8[\"diameter_breast_height\"].apply(lambda x: np.log(x))\n",
    "\n",
    "\n",
    "# or something else\n",
    "data_prep_8 = data_prep_7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference (if needed) evaluating distributions of numeric values\n",
    "# data_prep_7[''].dropna().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation (i.e. rescaling) \n",
    "\n",
    "# The standard approach is minmax scaling. Note that this does not handle null values.\n",
    "data_prep_9 = data_prep_8.copy()\n",
    "\n",
    "for c in cols_num:\n",
    "    data_prep_9[c].dropna(subset = [c], inplace=True)\n",
    "    data_prep_9[c] = minmax_scaling(data_prep_9[c],columns = [0])\n",
    "\n",
    "# Chris Albion recommends defaulting to standardization unless the model \n",
    "# demands otherwise\n",
    "\n",
    "# def scaler(x):\n",
    "# # Create scaler scaler = preprocessing.StandardScaler() # Transform the feature standardized = scaler.fit_transform( x) # Show feature standardized\n",
    "#     scaler = StandardScaler()\n",
    "#     return scaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References for Section 6\n",
    "[Min Max Scaling on Kaggle](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data) <br>\n",
    "Outliers function partially credited to Albon, Chris. Machine Learning with Python Cookbook: Practical Solutions from Preprocessing to Deep Learning (p. 70). O'Reilly Media. Kindle Edition. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleaning: Boolean Features\n",
    "<a id='Section7'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "Involves encoding Booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review bool cols values\n",
    "[print(c, data_prep_9[c].value_counts(), sep=\"\\n\") for c in cols_bool] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a replacement dict\n",
    "replace_bool = {col1:{value1:1,value2:0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values for ints to encode bool\n",
    "data_prep_10 = data_prep_9.copy()\n",
    "data_prep_10 = data_prep_10.replace(to_replace=replace_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review transformed bool cols values\n",
    "[print(c, data_prep_10[c].value_counts(), sep=\"\\n\") for c in cols_bool] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleaning: Date Features\n",
    "<a id='Section8'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section involves\n",
    "- Date Encoding\n",
    "- Date Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review date cols\n",
    "cols_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Date encoding - note that this can be very slow, so it sometimes can be worthwhile specifying the datetime format\n",
    "data_prep_11 = data_prep_10.copy()\n",
    "\n",
    "for col in cols_date:\n",
    "    data_prep_11[col] = pd.to_datetime(data_prep_11[col], infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date feature generation - for a tidy dataset, it can make sense to break out a date feature into week, month, and year. \n",
    "data_prep_11['year_'] = data_prep_11[''].dt.year\n",
    "data_prep_11['month_'] = data_prep_11[''].dt.month\n",
    "data_prep_11['week_'] = data_prep_11[''].dt.week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleaning: Categorical Features\n",
    "<a id='Section9'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "- Standardization\n",
    "- Cardinality Restriction\n",
    "- Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing Categorical Features and Values\n",
    "[print(col,data_prep_11[col].value_counts(),sep=\"\\n\") for col in cols_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing all text in categorical columns to protect against data entry errors\n",
    "punct_reg = re.compile('[%s+]' % re.escape(string.whitespace + string.punctuation))\n",
    "def text_proc(text):\n",
    "    proc = str(text)\n",
    "    proc = proc.lower() #changes case to lower\n",
    "    proc = proc.strip() #removes leading and trailing spaces/tabs/new lines\n",
    "    proc = punct_reg.sub('_', proc)\n",
    "    return proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_prep_12=data_prep_11.copy()\n",
    "for col in cols_cat:\n",
    "    data_prep_12[col] = data_prep_12[col].apply(lambda x: text_proc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating after transformation\n",
    "[print(col,data_prep_12[col].value_counts(),sep=\"\\n\") for col in cols_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical features\n",
    "# Ordinal categories can be handled through replace\n",
    "scale_mapper = {'new':1,\n",
    "                'juvenile':2,\n",
    "                'semi_mature':3,\n",
    "                'mature':4,\n",
    "                'over_mature':5\n",
    "               }\n",
    "data_prep_12[''] = data_prep_12[''].replace('scale_mapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal categories can be handled through one hot encoding or dummification\n",
    "data_prep_13 = pd.get_dummies(data_prep_12, prefix = None, prefix_sep = '-', dummy_na = False, columns = [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_13.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleaning: Other Features\n",
    "<a id='Section10'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "Section for ad hoc or specialised cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.e. for Geo features - obtaining postcode for each tree\n",
    "# Though there is a limit of 1 search per second!\n",
    "import sys\n",
    "!{sys.executable} -m pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = geolocator.reverse(\"-37.794463412577585, 144.93192049089112\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.raw['address']['postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.raw['address'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation of Prepared Data\n",
    "<a id='Section11'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "This section involves\n",
    "- profiling the cleaned data\n",
    "- Managing any remaining warnings\n",
    "- Saving the dataset in the HDF5 format\n",
    "- noting things to change based on the feedback from modelling (i.e. the MSE of estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile cleaned data\n",
    "report2 = pandas_profiling.ProfileReport(data_prep_13)\n",
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve warnings - this may involve imputation (some example code below)\n",
    "data_prep_14 = data_prep_13.copy()\n",
    "mean_imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0)\n",
    "mean_imputer_fit = mean_imputer.fit(data_prep_14[['diameter_breast_height_scaled','useful_life_expectency_value_scaled','age_description']])\n",
    "data_prep_14_imputed = pd.DataFrame(data = mean_imputer_fit.transform(data_prep_14[['diameter_breast_height_scaled','useful_life_expectency_value_scaled','age_description']].values)\\\n",
    "            ,index=data_prep_14.index \\\n",
    "            ,columns=data_prep_14[['diameter_breast_height_scaled','useful_life_expectency_value_scaled','age_description']].columns)\n",
    "data_prep_15 = \\\n",
    "pd.concat([data_prep_14_imputed,data_prep_14.drop(['diameter_breast_height_scaled','useful_life_expectency_value_scaled','age_description'],axis=1)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all warnings are resolved\n",
    "report3 = pandas_profiling.ProfileReport(data_prep_15)\n",
    "report3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to HDF5 format to preserve formatting\n",
    "%%time\n",
    "# Saving data to HDF5 format\n",
    "data_prep_15.to_hdf('data_prep_15.h5', key = 'data_prep_15', mode = 'w', append = True, format = 'table',\\\n",
    "          index = False, complib = 'blosc', optlevel = 9, data_columns = list(data_prep_15.index.names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Construction of a Prep Pipeline\n",
    "<a id='Section12'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "Once data transformations are finalized, it makes sense to group them into a pipeline, to allow for reproducible file handling <br>\n",
    "Functions are a great way to group this. <br>\n",
    "Below is an example block from another project. The next version of this workbook will build this out. <br>\n",
    "This workbook didn't touch much on dummification of categorical variables - so some code will be available for that as well. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example functions - note how they're abstracted to handle multiple dataframes.\n",
    "punct_reg = re.compile('[%s+]' % re.escape(string.whitespace + string.punctuation))\n",
    "\n",
    "def text_proc(text):\n",
    "    proc = str(text)\n",
    "    proc = punct_reg.sub('_', proc)\n",
    "    return proc\n",
    "\n",
    "def data_processing(df):\n",
    "    _data = df.copy()\n",
    "\n",
    "    _num_cols = [col for col in _data.columns[(_data.dtypes == np.float64) | (_data.dtypes == np.float32)]]\n",
    "    \n",
    "    _date_cols = list(_data.columns[_data.columns.str.contains('date')])\n",
    "    \n",
    "    _text_cols = [col for col in _data.columns[_data.dtypes == 'object'] if col not in _date_cols]\n",
    "    \n",
    "    \n",
    "    _bool_cols = [col for col in _text_cols if ('yes' in _data[col].values) or ('no' in _data[col].values) or\n",
    "                  ('true' in _data[col].values) or ('false' in _data[col].values) ]\n",
    "    \n",
    "    for col in _num_cols:\n",
    "        _data[col].fillna(0, inplace = True)\n",
    "    \n",
    "    for col in _date_cols:\n",
    "        \n",
    "        _data[col] = pd.to_datetime(_data[col], infer_datetime_format = True)\n",
    "    \n",
    "    for col in _text_cols:\n",
    "        _data[col] = _data[col].str.lower().apply(lambda x: text_proc(x))\n",
    "    \n",
    "    for col in _bool_cols:\n",
    "        _data[col] = _data[col].replace({ 'nan':0, 'no':0, 'yes':1, 'false':0, 'true':1})\n",
    "        _data[col] = _data[col].astype(np.float32)\n",
    "    \n",
    "    return _data\n",
    "\n",
    "def data_denoising(df):\n",
    "    _data = df.copy()\n",
    "    \n",
    "    _date_cols = list(_data.columns[_data.columns.str.contains('date')])\n",
    "\n",
    "    _text_cols = [col for col in _data.columns[_data.dtypes == 'object'] if col not in _date_cols]\n",
    "\n",
    "    _bool_cols = [col for col in _text_cols if ('yes' in _data[col].values) or ('no' in _data[col].values) or\n",
    "                  ('true' in _data[col].values) or ('false' in _data[col].values) ]\n",
    "    \n",
    "    feats_distros = dict()\n",
    "    for c in _text_cols:\n",
    "        _feat = _data[c]\n",
    "        _feat = _feat.value_counts()\n",
    "        _feat.fillna(0, inplace=True)\n",
    "        _feat = _feat.astype(np.int32)\n",
    "        _feat.sort_values(ascending = False, inplace = True)\n",
    "        _feat = pd.DataFrame(_feat)\n",
    "        _feat.columns = [c + ' count']\n",
    "        _feat[c + ' distribution'] = 100*_feat/_feat.sum()\n",
    "        feats_distros.update({c:_feat})\n",
    "\n",
    "    for feat in _text_cols:\n",
    "        feat_distro = feats_distros[feat][feat + ' distribution']\n",
    "        feat_index = feat_distro[feat_distro < 1].index.tolist()\n",
    "        len_feat_index = len(feat_index)\n",
    "        if len_feat_index > 0:\n",
    "            feat_sub = len_feat_index*[np.nan]\n",
    "            feat_dict = dict(zip(feat_index, feat_sub))\n",
    "            _data[feat] = _data[feat].replace(feat_dict)\n",
    "\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function call\n",
    "fy17fy18_event_2 = data_denoising(data_processing(fy17fy18_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Dummification (for analysis of text categories)\n",
    "%%time\n",
    "merged_dummies = pd.get_dummies(merged, prefix = None, prefix_sep = '-', dummy_na = False, columns = merged_text_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. References\n",
    "<a id='Section13'></a>\n",
    "[Go back to contents](#Section0) <br>\n",
    "- [Chris Albion’s blog](https://chrisalbon.com/) and [recently published book](https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386/ref=sr_1_1)\n",
    "- [Theadore Petrou’s Pandas Cookbook](https://www.packtpub.com/big-data-and-business-intelligence/pandas-cookbook)\n",
    "- Racheal Tatman’s [Data Cleaning Challenge](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values) and [blog](http://www.rctatman.com/)\n",
    "- Katharine Jarmul’s [Automating Data Cleanup with Python](https://www.youtube.com/watch?v=gp-ngPV_ZX8)\n",
    "- Data Camp’s [Importing and Cleaning data with Python](https://www.datacamp.com/tracks/importing-cleaning-data-with-python)\n",
    "- [Luis Daniel Lopez-Sanchez](https://www.linkedin.com/in/luis-daniel-l%C3%B3pez-s%C3%A1nchez-5b624864/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
